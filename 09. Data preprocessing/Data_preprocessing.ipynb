{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Data Preprocessing \n",
    "<p>Data: Single piece of data [Ram, Syam, Hari], photos of dog; cat etc. </p>\n",
    "<p>Data Set: Collection of data which is ready to be preprocessed for AI pipeline; Sabei sets of data, data is formated and placed and ready to be pre preprocessed. Table format of data and the varios data associated with the data. Folders of Dog, cat, birds is a dataset.</p>\n",
    "<p>A noble works requires data set to be made by oneself other wise kaggle. For superwise learning we need to modify features and learning \n",
    "<br/>For unsupervise learning we need to modify only features. This is done for a complementary new set of data. or done to make benchmark of dataset.</p>\n",
    "<p>House pricing dataset; flower dataset are easily avaliable so are easy to get but in cases of dataset which aren't easily avalliable then it is expensive </p>\n",
    "<p>Data set can be in : Excel file, CSV for Machine learning logistic, linear, timeseries(eaisest); Image jpeg,png; Natural Language processing: TXT, Docx; Sound: mp3, avi convert music to reconize sound pattern(spectogram analysis).<br/> Data engenier job is to see data sets or wharehouse of data and on the basis of data which pipeline is better and the core specilization of data engineer </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "<p>Datasets may have certain things missing like cat dog classification in such cases Cat has 1500 images and dog have 300 images this type of model is heavily biased. And only see one thing repeated multiple of time. As Nepali want to see snow compared to a Canadian. Here the model is able to reconize cat easily while dog may face dificulty</p>\n",
    "    - We can increase dog image through augumentaion crop, rotate upsampling \n",
    "    - Down scaling cat images oly choose 500 images.\n",
    "<p>This helps to handel imbalance situation</p>\n",
    "<p>Tabular data with missing data and in such type of data exclude such data and only choose relevant data to AI pipeline. And send the missing data to AI pipeline and in such cases AI fills after AI understands it already and fills it in. Missing data are common and discarding it is also common.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Catogerical Data\n",
    "<p>Computer only understands basics number only and dosen't understand Cat or Dog term. The final output are converted into number, is know as encoding. The term or object are assign a numberical value is encoding. Apple == 1 </p>\n",
    "\n",
    "    - Level encoding: Assign a number Cat = 1 and Dog = 2 and the output is either one or two and understand by oursellf.\n",
    "    \"output: ['Cat', 'Dog'] output: [1,2]\" Used for continous values.\n",
    "    - One Hot encodoing :output [cat, dog] output: [1,0] [0,1] # here 1 represents yes and 0 is no. Classifying task is used.\n",
    "<p>Multi object together </p>\n",
    "    - We use multi class encoding and multi level encoding<br/>\n",
    "    - the output function is play around Soft max.<br/>\n",
    "    - If two animal input a photo if Cat and dog then the output will be op: [1 ,0: 95%] for cat and [0 ,1: 85%] for dog Use panddas, tensorflow(usually), and sckt learn<br/>\n",
    "    - If confidence thresholed > 50% : print the values(cat, dog)<br/>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features scaling \n",
    "<p>Features: in english it are the attributes or characteristics like car, bike and cycle are different tor determine the object<br/>\n",
    "- In order to determine a car, bike cycle <br/>\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>Engine(input to Ai)</th>\n",
    "    <th>No of wheels</th>\n",
    "    <th>Steering Wheels</th>\n",
    "    <th>Which Vehichl(label: output of AI)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1(yes)</td>\n",
    "    <td>4</td>\n",
    "    <td>1</td>\n",
    "    <td>Car</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1(yes)</td>\n",
    "    <td>2</td>\n",
    "    <td>0(no)</td>\n",
    "    <td>Bike</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>0(no)</td>\n",
    "    <td>2</td>\n",
    "    <td>0(no)</td>\n",
    "    <td>cycle</td>\n",
    "  </tr>\n",
    "</table> \n",
    "</p>\n",
    "\n",
    "<p>Features are characteristics to determine what object it is. The above three features help diffirentiate the object. The output is on the basis of probability score. If the object isn't classified or given the features then the AI can't determine what the object is as object hasn't been train upon it. So basically,features (input) are characteristics that determine the object and labels (output) are the output in this case the objects that is determined on the basis of the features. </p>\n",
    "\n",
    "<p>Feature scaling(Can also do in labels.)</p>\n",
    "<p>Standardization(same format): All the data is int and if one str then it confuses the Algorythm and our work is standardization ie; convert all the datatype to a simillar datatype and string are to be encoded into number, int to int, float to float</p>\n",
    "<p>Normallization(same range/scale): If the difference of number is greater and in such case bring everything to a defined scale or range. In percentage, or number and AI considers it own valid range.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Spliting\n",
    "<p>Model overfitting and model underfitting problem in real world. And why data spliting is important.</p>\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>X1</th>\n",
    "    <th>X2</th>\n",
    "    <th>Y</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>0(trainning data)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td>1(validation data)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>1(train set till here)(training data)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>1(testing data)</td>\n",
    "  </tr>\n",
    "</table> \n",
    "<p>Creating a model that X1 and X2 value given predict Y</p>\n",
    "<p>If all data is taken and trained the model memorizes the data of the dataset. Saves the state and dosen't understand the underlying logic.</p>\n",
    "<p>Below the train set if out question is given then the data fails to predict; it's a case of overfitting. Real world rational thinking isn't there. The AI model also checks the data in the old information and checks only in that data. But in real world case it fails. in case of 1,1 it fails.(chattur) No logical thinking question (nepal college)</p>\n",
    "<p>How to handel overfitting?<br/>\n",
    "- Train test split (train val test): Our model all dataset isn't used for trainnig only some parts of the data should be used for testing; in prev case all three data were used, now only two data set used to understad pattern and tested with the validation data set after a pattern is identify everytime if the pattern is wrong then the model again train itself and validates with the validation dataset a logic question is asked. Notes for bujne process.<br/>\n",
    "- Validation and testing data are different.<br/>\n",
    "- Validation data are like exam where After tranning data are given ie learning from the classroom we understand patter and check with validation data what we learn ie we give exam to evaluate how we are learning evaluate with the validation data.<br/>\n",
    "- Testing data is finishing the college and testing is work base env learning use in real world a new data completely unseen. is testing data.</p>\n",
    "\n",
    "<p>Trainning data( AI model understand logic 80%- 60% data is given so AI understand logic and not memorizes) -> validation data(20% data is given exam of college may or may not be repeated) -> testing data(Job problem adapting to the job; new data given). If not done then the model is over fitting occurs.<br/>\n",
    "If the data is large then (60-20-20) is done while if data is low (80-20) is done ie only trainning and validation is done. Simple task needs low and complex need more data.</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
