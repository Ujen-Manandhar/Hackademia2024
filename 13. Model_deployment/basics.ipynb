{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cover today\n",
    "\n",
    "<p>How to train model and deploy model: this will covver how to save a model after being trained. and load the model. If a model needs to be deployed new then what the model earlier is all lost. So, model need to be trained from scratch again and again. Saving is done in a file with model archeticure(y=mx+c (linear model)) and parameters(m and c value are saved). So due to this either the parameters is saved or both are saved. Due to privacy concers are archetcure aren't saved. </p>\n",
    "<p>How to deploy? we use API to deploy AI; From API we can understand the record for every API hit. Fast API and Flask make API, Consideration and Cloud Deployment.</p>\n",
    "<p>Mlops sees the deployment side of deploying ML; The work of MLops knows what the model does and on the basis of that server of the ML model is made and deploy. If AI model fails then first it is reached by MLops like repair mecanic and then only the industry mecanic.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Machine Learning Models\n",
    "<ul>\n",
    "    <li>Saving and loding models</li>\n",
    "    <li>Deploying using Flask or FastAPI(basic API creation)</li>\n",
    "    <li>Practical considerations for model deployment</li>\n",
    "    <li>Introduction to cloud deployment options(Heroku, AWS, GCP)</li>\n",
    "</ul>\n",
    "<p>Flask make backend and Fast Api for making API</p>\n",
    "<p>API is a waiter. API recives request from user and passes to the server in the backend. And the API delivers the response from the server. App that requires net does this. Like in google maps when a user send a request through google maps app and the app use api to send request to main server and ping(message) the server. The process is done by google map server and passes the result to api and then sends to the user.</p>\n",
    "<img src = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSCuHWsQyRvWi43fU__aih9Lww0WL85f2bOwg&s'>\n",
    "<p>What we need is request and output is response.</p>\n",
    "<p>Done through local host, make server and ping and get response </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Saving and Loading Models\n",
    "<p>How to save a trained model to disk and load it back into memory for making predictions. This is cruical for deploying machine learning models in production enviroments.</p>\n",
    "<p>Steps</p>\n",
    "<ol>\n",
    "    <li>Train a machine learning model</li>\n",
    "    <li>Save the model using joblib or pickle</li>\n",
    "    <li>load the model from disk</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time model prediction: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "Run Time Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression # probability prediction of classifier\n",
    "import pickle # save archecticure and parameter \n",
    "\n",
    "# load dataset and split it \n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# train a logistic regression model\n",
    "model = LogisticRegression(max_iter=200, random_state=42) # the iteration goes to 200 fixes it self if problem occurs then fixes it self \n",
    "model.fit(X_train, y_train) # trained model.\n",
    "\n",
    "# make prediction with the runtime model\n",
    "predictions = model.predict(X_test)\n",
    "print(f'Run Time model prediction: {predictions}')\n",
    "print(f'Run Time Model Accuracy: {model.score(X_test,y_test):.2f}') # check preformance with x test and y_test with the prediction two decimal points display\n",
    "# till runtime \n",
    "\n",
    "#save the model to file using pickle \n",
    "with open('iris_logistic_regression.pkl', 'wb') as f: # create a file with 'wb' writing in binary and save it \n",
    "    pickle.dump(model, f) #dump model in f (file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model prediction: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "Loaded Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# load the model to a file using pickle\n",
    "with open('iris_logistic_regression.pkl', 'rb') as f: #pkl is standard convention and 'rb' read binary\n",
    "    loaded_model = pickle.load(f) #load archeticure and parameter\n",
    "\n",
    "# Make prediction with the loaded model\n",
    "# no information is lost \n",
    "predictions = loaded_model.predict(X_test)\n",
    "print(f'Loaded model prediction: {predictions}')\n",
    "print(f'Loaded Model Accuracy: {loaded_model.score(X_test, y_test):.2f}') #seeing accuracy \n",
    "# done to check preformance of the model to see if the both are equal \n",
    "# pickle save parameter and arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying models using Flask\n",
    "<p>Flask is a micro web framework written in Python. It's easy to set up and use, making it popular choice for deploying machine learning models as a web service</p>\n",
    "\n",
    "<ol>\n",
    "    <li>Create a flask app</li>\n",
    "    <li>load the trained model</li>\n",
    "    <li>create an API endpoint to handel prediction requests.</li>\n",
    "</ol>\n",
    "<p>Creating an API, </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.101.5:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.101.5 - - [22/Oct/2024 14:54:48] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# creating api\n",
    "from flask import Flask, request, jsonify #customer request. and jsonify output convert to JSON\n",
    "import joblib\n",
    "import os # work with operating system\n",
    "\n",
    "# initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# load the trained model\n",
    "model = joblib.load('iris_logistic_regression.pkl') #load model \n",
    "\n",
    "# define a route for prediction API\n",
    "@app.route('/predict', methods = ['POST']) #api end points #ip address / predict address is used then predict() function is used \n",
    "# backend \n",
    "def predict():\n",
    "    data = request.get_json(force=True) # take data in json format \n",
    "    predictions = model.predict([data['features']]) # send data to the model converts the json automatically\n",
    "    return jsonify({'prediction': int(predictions[0])})# return in json format \n",
    "\n",
    "# run the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=os.getenv('IP', '0.0.0.0')),\n",
    "    port = int(os.getenv('Port', 4444))\n",
    "    # creating our device as the server \n",
    "\n",
    "# backend for ai and desiging the API for the AI backend\n",
    "# running as the powerful computer backend and returns value of prediction.\n",
    "# not found is as it is an API and the front end ping to this ip \n",
    "# the second ip is used to connect to other isn the same network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Deploying Models using Fast API\n",
    "<p>Fast Api is modern, fast(high-performance) web framework for building APIs with python 3.7 based on standarad python type hints. Fast Api is a great optin for deploying machine learning models due to its speed and support for async programming.</p>\n",
    "<ol>\n",
    "    <li>Create a FastAPI app</li>\n",
    "    <li>load the trained model</li>\n",
    "    <li>create an API endpoint to handel prediction requests.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, conlist\n",
    "import joblib\n",
    "\n",
    "#initialize Fast API app\n",
    "app = FastAPI()\n",
    "\n",
    "# load the trained model\n",
    "model = joblib.load('iris_logistic_regression.pkl')\n",
    "\n",
    "# define requees model\n",
    "class Features(BaseModel):\n",
    "    features: conlist(float, min_length=4, max_length=4)# ensure featureslist has exactly 4 float values\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict(data: Features):\n",
    "    try:\n",
    "        # predict using the model\n",
    "        prediction = model.pre dict([data.features])\n",
    "        return {'prediction': int(prediction[0])}\n",
    "    except Exception as e:\n",
    "        # Handel errors during prediction\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "# needs uvicorn filename:app --reload # nagagite to file and write this command \n",
    "# fast api preferable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pratical Consideration fro model deployment \n",
    "<p>Key Consideratios: Scalability, security: api key for protection, monitoring: log guru, model updating<p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
