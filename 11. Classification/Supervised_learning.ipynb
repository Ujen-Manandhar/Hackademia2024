{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREV class\n",
    "<p>House price prediction: Don't go and take all the data (houe price precidction linear regression beginers); All the features(independent valriables aren't needed); One method of decreasing features is PCA (principle component analysis); Ploting the graph and of feature to the output and see if they are linear and only consider which have linear pattern associated with it.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align = 'Center'>Classification<h3>\n",
    "\n",
    "<p>Fundamentally in linear regression output were in continous format eg: Temperature certain prediction if the data has linear pattern, and continous pattern, Stock market prediction of prices of stock. Done for prediction of continous data 10.3, 111.1 actual average number.</p>\n",
    "<p>Classification on the other hand tells if the given the data the output belong to a certain class or not. Class can be anything ie Labels if a vehicle has 4 wheels and a stering wheel it belongs to the class of car. Also gives the accuracy from confunsion matrix. Classification has various algorithm. Like logistic regression, K- nearest(KNN), SVM, Decision Tree, Random forest. After we cover all regression and classification then we can say we have learned all AI in the world. As sometimes there is regression and sometime classification and in other cases both like in GAN, Diffusion model (regression gives the coordinates of image) after ploting the cordinates then we go to classification to check if the image is right or wrong (if wrong then the regression model continously improves).</p>\n",
    "<p>Assigning the class is classification, ie where the output lies depending upon the features. eg: If an image of snake is given assign the class reptile or mammal depending upon the features(data); if the model gives the detail of the snake then it is regression(continous values). For classifying/choosing values then classification(discrete)</p>\n",
    "<img src ='https://dasarpai.com/assets/images/dspost/dsp6069-What-is-GAN-Architecture.jpg'>\n",
    "<p>Regression predicts continous value of constilation(stars). First we have the real image and then a generated image from regression and put inside the discriminiator(classifier) and which sees if the two images are similar or not with output Y/N. If two images are similar then the classifier says Y but if N then the regression model becomes better. As it is supervised learning we can also say no or yes to the classifier to make the classifier better if the ans is different from the ground truth. Here in GAN the regression modell makes the classifier better and the classifier makes the regression better. Regression model tries to fool the classifier by saying it's the orginal image and classifier tries to become better. With symbotic relationship both become better.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explotraty Data Analysis(EDA)\n",
    "<p>Understanding of data</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will be working on Iris Flower Dataset.</p>\n",
    "\n",
    "<img src = 'https://i.ytimg.com/vi/1x0w5VLB4sg/hqdefault.jpg?v=5e994f11'>\n",
    "\n",
    "<p>Three type flower data set with certain description, from survey(data) classifictaion id preformed on the dataset. From the column of Sepal length, sepal width, petal length petal width,(features); the type(label) Setosa, Versicolor and virginica is determined on the basis of the features. Classification is used every where eg; auto driving car to classifiy what is what in the roads like cars, bike , pedistrians. and depending upon the classification it determises to go stop, go slowly. So, basically classification trains itself on features then depending upon the input the model defines a set of rules and gives output of a class for the input given and finalizes it </p>\n",
    "\n",
    "<p>A comment is given the AI needs to predict the comment to give; let's say 'good morning' was given then the AI classify into greeting and in case of greeting what response is to be given is done by regression that gives continous value predictied which is a word.</p>\n",
    "\n",
    "<p>Classification is discrete (finite data 1,2,3,4 can count the number); Works best with supervised learning; Evaluation metrics are different as in regression we can't find acurracy as sightly off then a huge difference in acurracy but in classification we can find acurracy.</p>\n",
    "\n",
    "<p>Email spam (spam or ham); Disease Diagnosis(disease present or absent); Image recognition(identifiying object in image)</p>\n",
    "<ul>\n",
    "    <li>Features</li>\n",
    "        <li>Sepal length</li>\n",
    "        <li>Sepal Width</li>\n",
    "        <li>Petal Width</li>\n",
    "        <li>Petal length</li>\n",
    "    <br/>\n",
    "    <li>Classes(labels)<li>\n",
    "        <li>Setosa</li>\n",
    "        <li>Versicolor</li>\n",
    "        <li>Virginica</li>\n",
    "    <br/>\n",
    "    <li>Sample of 150 observation(50 of each class)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the iris data set (built in sklearn for pratice)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2\n",
       "5                5.4               3.9                1.7               0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for data use iris.data and for column iris.features \n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "display(iris_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  ...  target    species\n",
       "0                  5.1               3.5  ...       0     setosa\n",
       "1                  4.9               3.0  ...       0     setosa\n",
       "2                  4.7               3.2  ...       0     setosa\n",
       "3                  4.6               3.1  ...       0     setosa\n",
       "4                  5.0               3.6  ...       0     setosa\n",
       "..                 ...               ...  ...     ...        ...\n",
       "120                6.9               3.2  ...       2  virginica\n",
       "121                5.6               2.8  ...       2  virginica\n",
       "122                7.7               2.8  ...       2  virginica\n",
       "123                6.3               2.7  ...       2  virginica\n",
       "124                6.7               3.3  ...       2  virginica\n",
       "\n",
       "[125 rows x 6 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the df has only the features \n",
    "# now for the classes(labels)\n",
    "iris_df['target'] = iris.target # converting to discrete insted of text #output data \n",
    "# where 0 = setosa 1= versicolor 2= virginica\n",
    "\n",
    "iris_df['species'] = iris_df['target'].apply(lambda x: iris.target_names[x]) # go to target column and apply fucntion (the apply method applys a function in the column\n",
    "# if iris target is 0 value then return target names index of zero value retun working like map function list(map(lambda x: iris.target_names[x] , iris_df['target']))\n",
    "\n",
    "# adding the sepcies name column to the csv file.\n",
    "# the apply method is a method of pandas/This function acts as a map() function in Python. It takes a function as an input and applies this function to an entire DataFrame\n",
    "\n",
    "# display the column and rows.\n",
    "iris_df.head(125)\n",
    "\n",
    "\n",
    "# model to predict 0,1,2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "\n",
    "<p>Acts as a bridge between Classification and regression; firstly the work is done of regression and then of classification. Previsoly in regression output was continous variable single dimension value which is infinite of continous in nature.</p>\n",
    "\n",
    "<p>It is used for binary classification task(which brings the continous output to a discrete number). It predictis the probability of an instance belonging to a particular class. It uses sigmoid function </p>\n",
    "<img src = 'https://miro.medium.com/v2/resize:fit:970/1*Xu7B5y9gp0iL5ooBj7LtWw.png'>\n",
    "<p>The sigmoid function brings the value of continous series to a discrete series. The output is 0 and 1, basically gives probability of being a certain class, the class with the higgest probability is selected by us. The probability isn't accuracy it is only giving the confidiecne upon somthing how much confidient the model is for that class. if 0.93 then our model is highly confident it is of that class. We choose the class in which our model is the most confident as the predicted value. So, 0.50 would mean our model 50% confidient that the flower is setosa.</p>\n",
    "<p>Sigmoid computes one class at a time confidence is given. Another function is soft max function/ uses one hot encoding done individually for all values to 0 to 1</p>\n",
    "<img src = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSbZAnN-wESpYW3EDgNCBNdDjVRUnsxjBLUWw&s'>\n",
    "<p>Sofmax calculates at once on the basis of the labels calculate the higgest probability label and choose the higgest confidence. Done all at once for the output/ basis of level encoding; here depending upon an input a output for the label is given and on the basis of it probability of all the output will be given </p>\n",
    "<p>On the basis of encoding we use the different function.</p>\n",
    "\n",
    "<p>We use the softmax as there are 3 labels(multiple outputs). If only one output then sigmoid function with a threshold of 0.5 or more.</p>\n",
    "<p>Ml- Sklearn and DL- Tensor Flow</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confustion Matrix\n",
    "<p>It helps to identify the prediction accuracy of the models.</p>\n",
    "<br/>\n",
    "<img src = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcToFf0Bj3RwKFIW2Guz07BPI9ggnnghbsIAmw&s'>\n",
    "<p>The ground truth has truth and false of the actual values, and our predicted model values.</p>\n",
    "<ul>\n",
    "    <li>Our model also predicts true and in actual it is true then <b>true positive.</b></li><br/>\n",
    "    <li>Our model tells its false and actual it is true it is <b>false negative</b>. false. Type 2 error female pregnant but telling she's not pregnant. Most dangerous</li><br/>\n",
    "    <li>Our model tells its true and actual it is false iit is <b>false positive</b>. false ho positive aye ni. Type 1 error man being pregnant</li><br/>\n",
    "    <li>Our model tells its negative and actual negative then it is <b>true negative</b></li><br/>\n",
    "</ul>\n",
    "<p>Accuracy is percentage of correct TP and TN by total of the prediction. Between (0 and 1) closer to 1 is good.</p>\n",
    "<p>Precision is the number of time TP was predicted by the model the actual how many times it was predicted ;eg: card being scanned TP is the number of time card was acessed and FP is the cases when card wasn't used but system accepts it. Hamro model le true kati choti predict gare cha actual ma kati % gare cha bhane precision ho. </p>\n",
    "<p>Recall chai actual ma true vaye ko cases ma hamro model le true kati choti bhandei theeyo. eg: fraud detection we use recall total fraud cases and how many times did our model predict the farud case</p> \n",
    "<p>Class is balance (50-50) photos of dog and cat in such case accuracy is perfect, but in imbalance situation (90-10) the accuracy is skew towards dogs and accuracy isn't good metric and as such use F-1 Score. F1 score measure on the basis of precision and recall ie; actual true hunna bela kati ko true cha(presision) and true predict gare thau ma kati ko true cha(recall). And gives the average depending upon different class each class will have its own average.Duta class wise farak herxa unlike acurracy</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression Implementation\n",
    "<p>Let's implement logistic regression on data set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split #isolate data \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# normalization/ standarzation bring into same scale of the inputs which makes the model unbiased so the values are in a certain range and scale so model gives similar importance of value to all\n",
    "# address(1- 15m) time (1-24 hrs)\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.str_('versicolor'), np.str_('setosa'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('virginica'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa')] \n",
      " [np.str_('versicolor'), np.str_('setosa'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Uttam/Documents/DSML/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# select features and target\n",
    "X = iris_df.drop(['target', 'species'], axis=1) \n",
    "# drops the target and species column from the dataframe\n",
    "y = iris_df['species']\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)\n",
    "\n",
    "# initialize the scaler\n",
    "scaler = StandardScaler() #bring to the same level\n",
    "\n",
    "# fit and transform the training data \n",
    "X_train_scaled = scaler.fit_transform(X_train) #if fit is used there is some kind of training\n",
    "# Scaling automatically identifies min and max on the basis of it scaling occurs\n",
    "# on the basis of maxium and minium value bring the value into a certain number range\n",
    "\n",
    "# print(f'Scaled X_Train: {X_train_scaled}') # the data is scaled from the original\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_scaled = scaler.transform(X_test) #transform because on the basis of the sacaling of x train the information is stored in scaler and as such we can use transform to the same level\n",
    "# fit transform isn't again used because test data may be different from the original training data and as such a different scaling may occur.\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(multi_class='ovr', solver='liblinear') \n",
    "# Ovr one vs the rest algorithm used , softmax can be used. # by default liblinear is taken slove approch \n",
    "\n",
    "#fit the model to the training data\n",
    "log_reg.fit(X_train_scaled,y_train)\n",
    "\n",
    "# predict on the test data\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(list(y_pred),'\\n',[elements for elements in y_test])\n",
    "# the confusion matrix measures on the basis of the pred and actual values calculates for all the three labels; ie while ground truth setos how many times setosa was predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted Iris Flower species is:  setosa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Uttam/Documents/DSML/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# user input and prediction \n",
    "sepal_length = float(input('Sepal length (in cm): '))\n",
    "sepal_width = float(input('sepal width (in cm): '))\n",
    "petal_length = float(input('petal length(in cm): '))\n",
    "petal_width = float(input('petal width (in cm): '))\n",
    "\n",
    "# create an array for the input values \n",
    "user_input = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "\n",
    "# scale the user input using the same scalar\n",
    "user_input_scaled = scaler.transform(user_input)\n",
    "\n",
    "# Predict the species using the trained model\n",
    "predicted_species = log_reg.predict(user_input_scaled)\n",
    "\n",
    "# output\n",
    "print('The predicted Iris Flower species is: ', predicted_species[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://www.researchgate.net/publication/365099641/figure/fig3/AS:11431281096367166@1668121079561/Confusion-Matrix-3x3.png'>\n",
    "\n",
    "<p>We look at the model from both column and row eg; here in case of versicolor (row) the ground truth is versicolor but model predicted virginica in such a case the model predicted false even if it was true so false negative. in case for virginica (column) the model predicted a false positive where even if the versicolor wasn't virginica the model got a false positive.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n",
      "\n",
      "\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.89      0.94         9\n",
      "   virginica       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      "\n",
      "Accuracy Score:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred) # ground truth, pred\n",
    "'''\n",
    "     Predicted\n",
    "    [[10  0  0] <- actual out of 10 setosa predicted 10 were actual\n",
    "    [ 0  8  1] = 9 <- out of 9 versicolor it predicted 8 versicolor and 1 virginica\n",
    "    [ 0  0 11]]\n",
    "           12 <- total\n",
    "    one false negative case actuall it is versicolor model tells it is virginica\n",
    "    # 30 values only because of test variable.\n",
    "'''\n",
    "\n",
    "print('Confusion Matrix: \\n', conf_matrix)\n",
    "print('\\n')\n",
    "# Classification Report\n",
    "class_report = classification_report(y_test,y_pred)\n",
    "print('Classification report: \\n', class_report)\n",
    "# correct positive/(total actual positive(row)) = 8/9 = 0.89 versicolor (recall)\n",
    "# correct positive/(total predicted positive(column))= 11/12 = 0.92 virginica (precision)\n",
    "\n",
    "# most important\n",
    "# see \n",
    "# setosa most accurate\n",
    "# if in a versicolor hoina vane cha bhane might be wrong \n",
    "# if it says its virginica then 92% chance it might be right; if no then 100 % chances of being right (see column and row to intepret even more)\n",
    "\n",
    "print('\\n')\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy Score: ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors (KNN) Classifier\n",
    "<p>KNN can preform better than logistic regression and more accurate but problem is ins't scalable if millions of data  </p>\n",
    "<img src = 'https://www.researchgate.net/publication/340945742/figure/fig2/AS:884913532325897@1587991235934/Representation-of-KNN-algorithm-12.ppm'>\n",
    "<p>After the classes has been identified based upon the data set, now in the model traning process If K = 3(the number of neighbour or data to look at and taken on the basis of trail and error by looking at points )it looks at the nearest neibhour from the point and classify itself now on the basis of the data based on the majority of the data it classify and it chooses the class of the majority data points; If any unknow points enters then it finds the closest neighbor by finding the distance of the points from the overall dataset, then only look at the nearest 3 neighbour then only majority is calculated for which class the points belong to and asign it self to that particular class. Initially, 3 to 4 points can go anywhere and as such each point is assumed a particular class and after a number of classes try to be similar then only actual classes are identified. And if wrong prediction then such case the points can be rearranged. </p>\n",
    "<p>KNN is used when classifier or classification is simple, non linear and can't be done with logistic regression then KNN is consider.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.str_('versicolor'), np.str_('setosa'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa')] \n",
      " [np.str_('versicolor'), np.str_('setosa'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('setosa'), np.str_('versicolor'), np.str_('virginica'), np.str_('versicolor'), np.str_('versicolor'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('setosa'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('virginica'), np.str_('setosa'), np.str_('setosa')]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN model with 5 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the model to the training data\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "\n",
    "# predict on the test data \n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(list(y_pred_knn),'\\n', list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "  [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "Accuracy Score\n",
      "  1.0\n"
     ]
    }
   ],
   "source": [
    "# display the model's performance \n",
    "print('\\nConfusion Matrix:\\n ', confusion_matrix(y_test, y_pred_knn))\n",
    "print('\\nClassification Report\\n ', classification_report(y_test, y_pred_knn))\n",
    "print('\\nAccuracy Score\\n ', accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector Machines\n",
    "#### Decision Trees \n",
    "<p>Support Vector Machine and Decision Tree Classifier Theoritical understanding and Implement it on Iris flower dataset</p>\n",
    "<p>For decision trees; completely random and learn rule; id3 algo</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
