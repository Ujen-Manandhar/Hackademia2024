{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Applications of AI - Natural Language Processing(NLP)\n",
    "#### Overview of NLP and its Applications\n",
    "<p>Natural Language Processing(NLP) is a branch of Ai that focuses on the interaction between computers and human languages. It involves the ability of a computer tor understand, interpret and generate human language in a valuable way. Computer understanding our day to day language like english.</p>\n",
    "<p>Common Application of NLP</p>\n",
    "<ul>\n",
    "    <li><b>Text Classification:</b> Spam detection(spam|ham), sentiment analysis</li>\n",
    "    <li><b>Machine Translation:</b> Translating tex from one languate to another (eg: google translate)</li>\n",
    "    <li><b>Name Entitiy recognition (NER):</b> Identifying proper names (adverb, adjective, noun, pronoun) in text and classifiying them into predefinded categories like names of persons organizations, address of the person etc</li>\n",
    "    <li><b>Speech Recognition:</b> Converting spoken languages into text. Siri, Gemini, cortana</li>\n",
    "    <li><b>Chatbots:</b> Automated system that can interact with users in Natural language</li>\n",
    "</ul>\n",
    "<p>We were first checking strings and this has restriction this causes problem as a person can't write all the rules (condition), ie rule based method. Too many if condition. Ie not ai as in ai makes its own rule. NLP came because we can't write all the rules. Chat gpt is a NLP process.GPT understand the sentences and breakdown the sentence and understands the nuances of text and from the data it has it organizes the output through NLP and gives output. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing packages\n",
    "Nltik and spacy are needed for this project<br/>\n",
    "Natural language toolkit and Spacy use alternative and has text corpus and pretrained model. Preprocessing of data. If a new model then sklearn and tensor flow.</br>\n",
    "Documentation and alternatives of the function should be seen<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Processing Techniques\n",
    "<p>Text processing is the foundation of NLP. We ogten need to clean and prepare text data before appling more complex models. Here are some basic text processing techniques. NLP has two ways from model training through Sklearn or tensor flow. This takes a lot of time as to understand text nuances(eat and ate, sleep or slept). Due to this simple task becomes difficult if we process the text a little for these nuances to their bare minium and isolate words, and remove unneeded stop words, period , comma. So to make the work bare minium. makes the processing easy </p>\n",
    "<p>Tokenization</p>\n",
    "<p>Tokenization is the process of breaking down text into individual units, sush as words or sentences. Isolates the words in the sentance individually.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Run here ---\n",
      "--------------------------------------------------\n",
      "Sentences: ['Natural Language Processing with Python is fun.', \"Let's tokenize this sentence!\"]\n",
      "Words: ['Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'fun', '.', 'Let', \"'s\", 'tokenize', 'this', 'sentence', '!']\n"
     ]
    }
   ],
   "source": [
    "# packages \n",
    "import spacy # provide text corpos , pre trained model, datasets (classified data set of the words.)\n",
    "from spacy.cli import download #from spacy command line interface \n",
    "\n",
    "#try loading the model and if it fails, download and install it\n",
    "#try\n",
    "nlp = spacy.load('en_core_web_sm') #load model (small model )\n",
    "\n",
    "# except OSError: # if model isn't install then download \n",
    "#     print(\"Model not found. Downloading...\")\n",
    "#     download('en_core_web_sm')\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     print('\\n'*4)\n",
    "\n",
    "# example text\n",
    "text = 'Natural Language Processing with Python is fun. Let\\'s tokenize this sentence!'\n",
    "\n",
    "print('Code Run here ---')\n",
    "print('--'*25)\n",
    "# process the text\n",
    "doc = nlp(text)# doc has the output of the process done by model\n",
    "\n",
    "# sentence tokenization\n",
    "sentences = [sent.text for sent in doc.sents] # identify the sentences\n",
    "print('Sentences:', sentences)\n",
    "\n",
    "# word Tokenization \n",
    "words= [token.text for token in doc] #identify the words in doc variable. \n",
    "print('Words:', words) # token = words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'fun.', \"Let's\", 'tokenize', 'this', 'sentence!']\n"
     ]
    }
   ],
   "source": [
    "print(text.split()) #the full stop isn't split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "<p>Stemming is the process of reducing words to their base root form.</p>\n",
    "<p>Tokenization was used for to isolate individual words. So words could be process easily and not heavy in the system. Stemming is making the word a bare minium (diven to drive) simple words.</p>\n",
    "<p>The tokenization was done in order to make the process easier and steming is done to make the process to the bare minium and why NLTK isn't used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'ran', 'runner', 'easili', 'fairli', 'eat', 'drive', 'riding ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# initialize the stemmer \n",
    "ps = PorterStemmer() # stem model \n",
    "\n",
    "# example words\n",
    "words = ['running', 'ran', 'runner', 'easily', 'fairly' , 'eating', 'driving', 'riding ']\n",
    "\n",
    "# stem the words\n",
    "stems = [ps.stem(word) for word in words] #convert words to minium format\n",
    "print(stems)\n",
    "# here the nltk model ins't able to stem all the words and there is a distance of the words. \n",
    "# spacy is up to date and is able to figure out such words.\n",
    "# helps in the bag of words(vectorization) algo to make processing faster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "<p>Lemmatization is similar to stemming but it brings context to the words, reduciong them to their base form using a dictionary. Going to the base form make processing easier</p>\n",
    "<p>Dictionary has a  root word (key word) and synonam other base words. 'run' and 'ran' two words 'ran' is a base word and there are similar words similar to the word 'ran' like 'running' and represent to the root word of this which is run. </p>\n",
    "<p> {'eat' : [ ate, eating, eaten, and many more]} <- main dict<br/>\n",
    "he [ate] -> is the base word in minium format is 'ate'-> and search in the dict for the root word and convert it to the primary root word 'eat/ big bowl of rice </p>\n",
    "<p>so new sentence is `he eat big bowl of rice` </p>\n",
    "<p>First this dict is created by the user and then the model learns and make the dict</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas:  ['Natural', 'Language', 'processing', 'with', 'Python', 'be', 'fun', '.', 'let', 'us', 'tokenize', 'this', 'sentence', '!']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize words\n",
    "lemmas = [token.lemma_ for token in doc] # here the token was a variable was converted in object and used the method of leema_ on the object\n",
    "print('Lemmas: ', lemmas)\n",
    "\n",
    "# We don't care about grammar now only individual words.\n",
    "# is -> be the root word of is == be\n",
    "# He is [be] crazy # primary word is be \n",
    "# They are [be] crazy\n",
    "# converts to the primary root word of the base word. Make the context same \n",
    "# let's -> let us \n",
    "# that's -> that be [is] #this is understood by the model itself # similar to having a  second hand bike rather than making the bike by yourown self.\n",
    "# if we have a local data set then everything needs to be done by ourself like making the bike from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis \n",
    "<p>Sentiment analysis is the process of determining whether a piece of text is positive, negative or netural.</p>\n",
    "<p>Approach:</p>\n",
    "<ul>\n",
    "    <li>We'll use a pre-trained model or a basic logistic regression model for sentiment analaysis.</li>\n",
    "    <li>The model will be trained on text data, and we'll use it to predict the sentiment of new sentences.</li>\n",
    "</ul>\n",
    "<p>In this we have different class(labels) with different sentences and using bag or words(count vectorizor) in order to convert individual words into number. Now we have features represented by the numbers (words converted) (x1,x2,x3,x4) which is text with various numbers pattern and righ next to the pattern there is label (output) </p>\n",
    "<p>'good morning how are you?' are seperated in their base form and converted into number pattern for each words through vectorization (converting into vector of number). This sequence of number pattern has an output so in this case it is a greeting. And another sequence of number patter represents 'Good bye!!' then they have the common word good on the basis of this the model is learning and finding out differences and patterns in the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have dataset avaliabe already with steming and lemitazition now needs to be vectorized (making words to number)\n",
    "# the method of vectorization used is count vectorization that replaces the words the number of time it is repeated. \n",
    "# if ride is repeated 15x then ride is replaced by 15.\n",
    "# if we have a custom data set we need to do tokenization and lemmatization do by our self ; but in case of below project everything is already done \n",
    "\n",
    "# importing the packages.\n",
    "import ssl\n",
    "import certifi\n",
    "import pandas as pd #look at the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer # bag of words number repeating the words; numberical form conversion # vectorization has different algorithm we can even do through asci character \n",
    "# a word can also be converted in  higher dimension  of 3d or more. Helps to identify the distance and similarty of the words. and also create classification model. \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.datasets import fetch_20newsgroups # automatically give us the dataset with features text and labels.\n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: presiden@fraser.sfu.ca (Pat Presidente)\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: dsg@ecrc.de (Douglas S. Greer)\\nSubject:...</td>\n",
       "      <td>0</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: eggertj@moses.ll.mit.edu (Jim Eggert x61...</td>\n",
       "      <td>9</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: smb@col.hp.com (Sam Bauer)\\nSubject: &gt;&gt;F...</td>\n",
       "      <td>3</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: ejs16@cunixb.cc.columbia.edu (Eric Jaron...</td>\n",
       "      <td>3</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: hoss@panix.com (Felix the Cat)\\nSubject:...</td>\n",
       "      <td>7</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  From: presiden@fraser.sfu.ca (Pat Presidente)\\...         1   \n",
       "1  From: dsg@ecrc.de (Douglas S. Greer)\\nSubject:...         0   \n",
       "2  From: eggertj@moses.ll.mit.edu (Jim Eggert x61...         9   \n",
       "3  From: smb@col.hp.com (Sam Bauer)\\nSubject: >>F...         3   \n",
       "4  From: ejs16@cunixb.cc.columbia.edu (Eric Jaron...         3   \n",
       "5  From: hoss@panix.com (Felix the Cat)\\nSubject:...         7   \n",
       "\n",
       "                     names  \n",
       "0  comp.os.ms-windows.misc  \n",
       "1            comp.graphics  \n",
       "2    talk.politics.mideast  \n",
       "3             misc.forsale  \n",
       "4             misc.forsale  \n",
       "5                  sci.med  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Use certifiâ€™s certificate bundle\n",
    "ssl._create_default_https_context = ssl.create_default_context\n",
    "ssl._create_default_https_context().load_verify_locations(certifi.where())\n",
    "\n",
    "# loading the various categories\n",
    "categories = [ \n",
    "    'rec.autos',\n",
    "    'sci.electronics',\n",
    "    'comp.graphics',\n",
    "    'rec.sport.hockey',\n",
    "    'talk.politics.guns',\n",
    "    'talk.politics.mideast',\n",
    "    'comp.os.ms-windows.misc',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'misc.forsale',\n",
    "    'sci.med'\n",
    "] #pre defined categories \n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='test', categories=categories) #avaliable 20 news of the category which is avalivabe in sklearn \n",
    "X, y = newsgroups.data, newsgroups.target # x features y labels \n",
    "target_names = newsgroups.target_names # get the categories names y is numbers and this is their names\n",
    "df = pd.DataFrame({'text': X, 'category': y})\n",
    "\n",
    "target_col = df['category'].apply(lambda x : target_names[x])\n",
    "df = pd.DataFrame({'text': X, 'category': y, 'names': target_col})\n",
    "\n",
    "display(df.head(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Compressed Sparse Row sparse matrix of dtype ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Compressed Sparse Row sparse matrix of dtype ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Compressed Sparse Row sparse matrix of dtype ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Compressed Sparse Row sparse matrix of dtype ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;Compressed Sparse Row sparse matrix of dtype ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;Compressed Sparse Row sparse matrix of dtype ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  <Compressed Sparse Row sparse matrix of dtype ...         1\n",
       "1  <Compressed Sparse Row sparse matrix of dtype ...         0\n",
       "2  <Compressed Sparse Row sparse matrix of dtype ...         9\n",
       "3  <Compressed Sparse Row sparse matrix of dtype ...         3\n",
       "4  <Compressed Sparse Row sparse matrix of dtype ...         3\n",
       "5  <Compressed Sparse Row sparse matrix of dtype ...         7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text vectorization\n",
    "vectorizer = CountVectorizer(stop_words='english') #convert individual words into number and remove stop words (., !, the, a, umm, an) as they may have high value due to vectorization.\n",
    "X_vect = vectorizer.fit_transform(X) # the above text has words now X_vect is a number \n",
    "df = pd.DataFrame({'text': X_vect, 'category': y})\n",
    "display(df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Trian test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)  # set max_iter to avoid convergence warnings\n",
    "model.fit(X_train, y_train)\n",
    "# the model uses 1000 iteration \n",
    "# the acurracy score is different as initially m and c value are random and converges to a point here due to bad luck the starting value may be bad and the model takes time\n",
    "# due to the high time of converging of the line to center were it best represent the data it may take more or less iteration to get better accuracy.\n",
    "# converge means curve fitting how long it reach the best fit line \n",
    "# max iteration can be increased for the line to converge.\n",
    "\n",
    "# test model \n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred) #confunsion matrix if data isn't balance then f1 socre\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the sentiment analysis model\n",
    "<p> You can input yout own sentence to see the model's prediction</p>\n",
    "<p>Taking the input and see where the input lies. in the catogery</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence: \"i bought new play station the graphics is very cool it runs many games in high fps with ultra hd views.\"\n",
      "Predicted Category: comp.graphics\n",
      "\n",
      "\n",
      "Sentence: \"I love playing hockey, its a good sport whrer we run after a single ball with everyone having a hockey stick equivalant to samurai sword.\"\n",
      "Predicted Category: rec.sport.hockey\n",
      "\n",
      "\n",
      "Sentence: \"Guns are very good in school it helps shooting and no politician hate shooting\"\n",
      "Predicted Category: sci.med\n"
     ]
    }
   ],
   "source": [
    "# exampe prediction \n",
    "new_sentences = [ input('Enter sentence to check: '), \n",
    "                 'I love playing hockey, its a good sport whrer we run after a single ball with everyone having a hockey stick equivalant to samurai sword.']\n",
    "new_x_vect = vectorizer.transform(new_sentences)\n",
    "predictions = model.predict(new_x_vect)\n",
    "# at times our scratch model works bad may be due to vectorizor or the model it self\n",
    "\n",
    "# print prediction with class names\n",
    "for sentence, prediction in zip(new_sentences, predictions):\n",
    "    print('\\n')\n",
    "    print(f'Sentence: \"{sentence}\"')\n",
    "    print(f'Predicted Category: {target_names[prediction]}')\n",
    "\n",
    "# prints the topic from what it is related to\n",
    "# so the input word vector and the catogery word vector relation is identified.\n",
    "# words features are identified and labels are given \n",
    "# so the count vectorizor checks the words and from the features sees the number of graphics related features are most repeating and thus classify the words on the basis of this \n",
    "# as there are more graphics reated words in the sentence the model identifies the category on the basis of numbers \n",
    "# this is basis of how many words are repeating and priority is given and as such category are classified.\n",
    "\n",
    "# This can cause a problem an as such use TFIDF (better approch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "<p>Words embeddings are dense vector representations of words. Unlike one- hot encedong embeddings capture the semantic meaning of words. Words embeddings are higher dimension vector 3d or more. More detail vector. 90% uses word embedings rather then count vectorizor. </p>\n",
    "<p>Common words embedding Techniques: </p>\n",
    "<ul>\n",
    "    <li>Word2vec : predicts the contex of a word in a sentecne or the word from its context</li>\n",
    "    <li>GloVe (global vector): A count-based model that creates embeddings by aggrgating global then semantic meanings of words.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Let's use pre-trained word embeddings with spaCy.</p>\n",
    "<p>Use word medium and for any word bring out the word embedings. 3d vector of the word. and if we have two 3d vector of the word we can check how similar the words are. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between king and queen : 0.38\n"
     ]
    }
   ],
   "source": [
    "# downlod medium model\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# exampele words \n",
    "word1 = nlp('king') #12 numbers\n",
    "word2 = nlp('queen') # 9 numbers and cosine similarity on the basis of distance less distance then similar and vice versa \n",
    "\n",
    "# calculate similarty\n",
    "similarity = word1.similarity(word2)\n",
    "print(f'Similarity between king and queen : {similarity:.2f}')\n",
    "#used in news article, post, recommendication system. \n",
    "# movie summary, spiderman, and spiderman 3 movie summary and another x-men summary which is similar and then which are similar are given as recommendition \n",
    "# kgf and kgf 2 similarity \n",
    "# tweet similarity \n",
    "# super hero movie summary and recommend system basis of similarity \n",
    "# every word vector are created. and converted to number and then see similarity basis of distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizig word embeddings \n",
    "\n",
    "You can use t-SNE or PCA to visualize the embeddings in 2d space "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
